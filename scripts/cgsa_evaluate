#!/usr/bin/env python
# -*- coding: utf-8; mode: python; -*-

#########################################################################
# This script is a modified version of SemEval-2013 scorer.
#
# Scorer for the SEMEVAL-2013 Task 2: Twitter Sentiment Analysis
# Author: ves@cs.jhu.edu
#
# This script takes a prediction file and a gold standard file
# (i.e., tweeti-a.dist.tsv) and produces scores. The prediction file
# should be in the same format as the gold standard file.
#
#########################################################################
from __future__ import absolute_import, unicode_literals, print_function

from glob import iglob
import  sklearn.metrics as sklm
import pandas as pd
import os
import sys

##################################################################
# Variables and Classes
ENCODING = "utf-8"
ID = "id"
POLARITY = "polarity"
TOKS = "tokens"
LEMMAS = "lemmas"
TAGS = "tags"
DEPS = "dependencies"
FEATS = "features"
POSITIVE = "positive"
NEGATIVE = "negative"
NEUTRAL = "neutral"
MACRO_CLASSES = [POSITIVE, NEGATIVE]
MICRO_CLASSES = [POSITIVE, NEGATIVE, NEUTRAL]


##################################################################
# Methods
def read_file(fname):
    """Read input file and construct DataFrame.

    Args:
      fname (str): path to the input file

    Returns:
      pd.DataFrame: tabular view of file's contents

    """
    assert os.path.exists(fname) and os.access(fname, os.R_OK), \
        "Cannot read file {!r}.".format(fname)
    ret = pd.read_table(fname,
                        names=[ID, POLARITY, TOKS, LEMMAS, TAGS, DEPS, FEATS],
                        header=None, skipinitialspace=True, encoding=ENCODING)
    # only leave data with labels (positive, negative, or neutral)
    ret = ret.loc[lambda df: df.polarity.isin(MICRO_CLASSES)]
    ret.sort_values(by=[ID], axis=0, inplace=True, kind="heapsort")
    return ret


def read_dir(fnames):
    """Read multiple input files and construct single DataFrame.

    Args:
      fnames (list[str]): paths to the input files

    Returns:
      pd.DataFrame: tabular view of files' contents

    """
    dframes = []
    for fname_i in fnames:
        os.path.exists(fname_i) and os.access(fname_i, os.R_OK), \
            "Cannot read file {!r}.".format(fname_i)
        dframes.append(read_file(fname_i))
    return pd.concat(dframes)


def read_data(path, glob_ptrn):
    """Read file or directory containing input data.

    Args:
      path (str): file or directory containing input data
      glob_ptrn (str): globbing pattern to use for finding files

    Returns:
      pd.DataFrame: tabular view of files' contents

    """
    if os.path.isdir(path):
        return read_dir(iglob(os.path.join(path, glob_ptrn)))
    return read_file(path)


def compute_stat(gold, pred, verbose):
    """Compute and output classification statistics.

    Args:
      gold (pd.DataFrame): annotated gold data
      pred (pd.DataFrame): predicted data
      verbose (bool): verbosity flag

    Returns:
      void:

    """
    y_gold_full = gold.loc[:, POLARITY]
    y_pred_full = pred.loc[:, POLARITY]

    print("General Statistics:")
    print(sklm.classification_report(y_gold_full, y_pred_full),
          end="\n\n")

    # Accuracy
    print("Accuracy: {:.2%}".format(
        sklm.accuracy_score(y_gold_full, y_pred_full)
    ))

    # Macro-Averaged F1 Score
    y_gold_pos_neg = gold.loc[lambda df: df.polarity.isin(MACRO_CLASSES),
                              POLARITY]
    y_pred_pos_neg = pred.loc[lambda df: df.polarity.isin(MACRO_CLASSES),
                              POLARITY]
    print(
        "Macro-Averaged F1-Score (Positive and Negative Classes):"
        " {:.2%}".format(
            sklm.f1_score(y_gold_pos_neg, y_pred_pos_neg, average="macro")
        ))

    # Micro-Averaged F1 Score
    print(
        "Micro-Averaged F1-Score (All Classes): {:.2%}".format(
            sklm.f1_score(y_gold_full, y_pred_full, average="micro")
        ), end="\n\n")

    # Confusion Matrix and Examples
    if verbose:
        print("Confusion Matrix:")
        print(sklm.confusion_matrix(
            gold.loc[:, POLARITY], pred.loc[:, POLARITY]))
        print("")

        print("Examples:")
        for y_gld, y_pred, df in zip(y_gold_full, y_pred_full, gold):
            if y_gld != y_pred:
                print(
                    "<<<\tgold:\t{:s}\n>>>\tpredicted:\t{:s}"
                    "\n{:s}\t{:s}\n".format(
                        y_gld, y_pred, df[ID], df[TOKS]
                    ).encode(ENCODING)
                )


def main(argv):
    """Main method for evaluating coarse-grained sentiment analysis.

    Args:
      argv (list[str]): CLI arguments

    """
    from argparse import ArgumentParser
    argparser = ArgumentParser(description="")
    argparser.add_argument("-g", "--glob-ptrn",
                           help="globbing pattern to use for finding files in"
                           " directories", type=str, default="*.tsv")
    argparser.add_argument("-v", "--verbose",
                           help="output prediction errors",
                           action="store_true")
    argparser.add_argument("gold_file",
                           help="file or directory containing gold data")
    argparser.add_argument("pred_file",
                           help="file or directory containing predicted"
                           " labels")
    args = argparser.parse_args(argv)

    gold_data = read_data(args.gold_file, args.glob_ptrn)
    pred_data = read_data(args.pred_file, args.glob_ptrn)

    # check whether we have the same set of ids in both datasets
    gold_ids = set(gold_data.loc[:, ID])
    pred_ids = set(pred_data.loc[:, ID])
    xor_ids = (gold_ids - pred_ids) | (pred_ids - gold_ids)
    assert not xor_ids, "Unmatched ids: {!r}".format(xor_ids)

    # compute and output classification statistics
    compute_stat(gold_data, pred_data, args.verbose)


##################################################################
# Main
if __name__ == "__main__":
    main(sys.argv[1:])
