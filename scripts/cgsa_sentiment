#!/usr/bin/env python
# -*- mode: python; coding: utf-8 -*-

##################################################################
# Imports
from __future__ import absolute_import, unicode_literals, print_function

from argparse import ArgumentParser
import codecs
import sys

from cgsa.cgsa import SentimentAnalyzer
from cgsa.common import LOGGER
from cgsa.constants import DFLT_MODEL_PATH, ENCODING, MOHAMMAD, SEVERYN, \
    TABOADA, DFLT_AUTO_LEXICA, DFLT_MANUAL_LEXICA
from cgsa.data import Tweet


##################################################################
# Variables and Constants
TRAIN = "train"
TEST = "test"


##################################################################
# Methods
def _add_cmn_options(a_parser):
    """Add common options to option subparser

    Args:
      a_parser (argparse.ArgumentParser):
        option subparser to which new options should be added

    Returns:
      void:

    """
    a_parser.add_argument("-m", "--model",
                          help="path to the main model (if different from"
                          " default)", type=str, default=DFLT_MODEL_PATH)
    a_parser.add_argument(
        "files", help="input file(s)", type=str, nargs="+")


def _read_data(files):
    """Read files and return an iterator over tweets.

    Args:
      files (list[str]): list of input files

    """
    if files is None:
        raise StopIteration
    for ifname in files:
        with codecs.open(ifname, 'r', ENCODING) as ifile:
            for iline in ifile:
                iline = iline.strip()
                if not iline:
                    continue
                yield Tweet(iline)


def main(argv):
    """Main method for training and applying sentiment classifiers.

    Args:
      argv (list[str]): CLI arguments

    Returns:
      int: 0 on success, non-0 otherwise

    """
    argparser = ArgumentParser(
        description="Determine polarity of the given messages.")
    subparsers = argparser.add_subparsers(
        help="type of operation to perform", dest="mode")

    parser_train = subparsers.add_parser(
        TRAIN, help="train model on the provided data")
    parser_train.add_argument("-t", "--type",
                              help="type(s) of the model(s) to train",
                              choices=(MOHAMMAD, SEVERYN, TABOADA),
                              required=True, type=str, action="append")
    parser_train.add_argument("-d", "--dev",
                              help="development data",
                              action="append")
    parser_train.add_argument("-g", "--grid-search",
                              help="determine best hyper-parameters using"
                              " grid search", action="store_true")
    parser_train.add_argument("--auto-lexicon",
                              help="automatic lexicon(s) to use",
                              action="append", default=DFLT_AUTO_LEXICA)
    parser_train.add_argument("--manual-lexicon",
                              help="manual lexicon(s) to use",
                              type=str, action="append",
                              default=DFLT_MANUAL_LEXICA)
    _add_cmn_options(parser_train)

    parser_test = subparsers.add_parser(
        TEST, help="determine polarity of the given messages")
    parser_test.add_argument("-b", "--batch",
                             help="batch-predict the classes",
                             action="store_true")
    _add_cmn_options(parser_test)

    args = argparser.parse_args()
    if args.mode == TRAIN:
        LOGGER.debug("Reading training set...")
        train_set = [tweet_i
                     for tweet_i in _read_data(args.files)]
        LOGGER.debug("Reading development set...")
        dev_set = [tweet_i
                   for tweet_i in _read_data(args.dev)]
        LOGGER.debug("Initialining analyzer...")
        analyzer = SentimentAnalyzer(args.type,
                                     auto_lexicons=args.auto_lexicon,
                                     manual_lexicons=args.manual_lexicon
                                     )
        LOGGER.debug("Training analyzer...")
        analyzer.train(train_set, dev_set,
                       a_path=args.model,
                       a_grid_search=args.grid_search
                       )
        LOGGER.debug("Saving analyzer...")
        LOGGER.debug("Finished.")
    else:
        analyzer = SentimentAnalyzer.load(args.model,
                                          on_demand=args.batch)
        if args.batch:
            tweets = [tweet_i
                      for tweet_i in _read_data(args.files)]
            analyzer.batch_predict(tweets)
            for tweet_i in tweets:
                print(unicode(tweet_i).encode(ENCODING))
        else:
            for tweet_i in _read_data(args.files):
                tweet_i.label = analyzer.predict(tweet_i)
                print(unicode(tweet_i).encode(ENCODING))


##################################################################
# Main
if __name__ == "__main__":
    main(sys.argv[1:])
