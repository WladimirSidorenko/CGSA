#!/usr/bin/env python
# -*- mode: python; coding: utf-8 -*-

##################################################################
# Imports
from __future__ import absolute_import, unicode_literals, print_function

from argparse import ArgumentParser
import codecs
import logging
import sys

from cgsa.cgsa import SentimentAnalyzer
from cgsa.utils.common import LOGGER
from cgsa.constants import (
    COND_PROB_FILE, DFLT_MODEL_PATH, DFLT_W2V_PATH, ENCODING, BAZIOTIS,
    BILSTM, GAMON, GUENTHER, HU_LIU, JUREK, KOLCHYNA, MOHAMMAD, MUSTO,
    MVRNN, RNN, RNTN, SEVERYN, TABOADA, YESSENALINA, THREE_CLASS)
from cgsa.utils.data import Tweet


##################################################################
# Variables and Constants
DEBUG = "debug"
TRAIN = "train"
TEST = "test"


##################################################################
# Methods
def _add_cmn_options(a_parser):
    """Add common options to option subparser

    Args:
      a_parser (argparse.ArgumentParser):
        option subparser to which new options should be added

    Returns:
      void:

    """
    a_parser.add_argument("-m", "--model",
                          help="path to the main model (if different from"
                          " default)", type=str, default=DFLT_MODEL_PATH)
    a_parser.add_argument(
        "files", help="input file(s)", type=str, nargs="+")


def _add_cmn_test_options(a_parser):
    """Add common options to option subparser

    Args:
      a_parser (argparse.ArgumentParser):
        option subparser to which new options should be added

    Returns:
      void:

    """
    _add_cmn_options(a_parser)
    a_parser.add_argument("--w2v-path",
                          help="path to pre-trained word2vec embeddings"
                          " to use (should better be the same as the ones "
                          "used for training)", type=str,
                          default=DFLT_W2V_PATH)
    a_parser.add_argument("-b", "--batch",
                          help="batch-predict the classes",
                          action="store_true")


def _read_data(files, train=False):
    """Read files and return an iterator over tweets.

    Args:
      files (list[str]): list of input files
      train (bool): train mode (skip unknown labels)

    """
    if files is None:
        raise StopIteration
    for ifname in files:
        with codecs.open(ifname, 'r', ENCODING) as ifile:
            for i, iline in enumerate(ifile):
                iline = iline.strip()
                if not iline:
                    continue
                try:
                    tweet = Tweet(iline)
                except:
                    print("Line #{:d}".format(i + 1), file=sys.stderr)
                    raise
                if tweet.label not in THREE_CLASS:
                    LOGGER.warn("Skipping tweet with label %s.",
                                tweet.label)
                    continue
                yield tweet


def main(argv):
    """Main method for training and applying sentiment classifiers.

    Args:
      argv (list[str]): CLI arguments

    Returns:
      int: 0 on success, non-0 otherwise

    """
    argparser = ArgumentParser(
        description="Determine polarity of the given messages.")
    argparser.add_argument("-v", "--verbose", help="debug mode",
                           action="store_true")
    subparsers = argparser.add_subparsers(
        help="type of operation to perform", dest="mode")

    parser_train = subparsers.add_parser(
        TRAIN, help="train model on the provided data")
    parser_train.add_argument("-t", "--type",
                              help="type(s) of the model(s) to train",
                              choices=(BAZIOTIS, BILSTM, GAMON, GUENTHER,
                                       HU_LIU, JUREK, KOLCHYNA, MOHAMMAD,
                                       MUSTO, MVRNN, RNN, RNTN, SEVERYN,
                                       TABOADA, YESSENALINA),
                              required=True, type=str, action="append")
    parser_train.add_argument("-c", "--cond-probs",
                              help="file containing conditional probabilities"
                              " of lexicon terms (used in the method of Jurek"
                              " et al.)", type=str, default=COND_PROB_FILE)
    parser_train.add_argument("-d", "--dev",
                              help="development data",
                              action="append")
    parser_train.add_argument("-g", "--grid-search",
                              help="determine best hyper-parameters using"
                              " grid search", action="store_true")
    parser_train.add_argument("-l", "--lexicon", help="lexicon(s) to use",
                              type=str, action="append", default=[])
    w2v_group = parser_train.add_mutually_exclusive_group()
    w2v_group.add_argument("--w2v", help="use pre-trained word2vec embeddings",
                           action="store_true")
    w2v_group.add_argument("--lstsq",
                           help="perform post-hoc mapping from standard "
                           "word2vec embeddings to task-specific vectors"
                           " using least squares (implies --word2vec)",
                           action="store_true")
    parser_train.add_argument("--w2v-path",
                              help="path to pre-trained word2vec embeddings"
                              " to use", type=str, default=DFLT_W2V_PATH)
    _add_cmn_options(parser_train)

    parser_test = subparsers.add_parser(
        TEST, help="determine polarity of the given messages")
    _add_cmn_test_options(parser_test)
    parser_debug = subparsers.add_parser(DEBUG,
                                         help="explain model's prediction")
    _add_cmn_test_options(parser_debug)

    args = argparser.parse_args()
    if args.verbose:
        log_lvl = logging.DEBUG
        LOGGER.setLevel(log_lvl)
        for handler_i in LOGGER.handlers:
            handler_i.setLevel(log_lvl)

    if args.mode == TRAIN:
        LOGGER.debug("Reading training set...")
        train_set = [tweet_i
                     for tweet_i in _read_data(args.files, True)]
        LOGGER.debug("Reading development set...")
        dev_set = [tweet_i
                   for tweet_i in _read_data(args.dev, True)]
        LOGGER.debug("Initializing analyzer...")
        analyzer = SentimentAnalyzer(args.type,
                                     lexicons=args.lexicon,
                                     cond_probs=args.cond_probs,
                                     w2v=args.w2v,
                                     lstsq=args.lstsq,
                                     w2v_path=args.w2v_path
                                     )
        LOGGER.debug("Training analyzer...")
        analyzer.train(train_set, dev_set,
                       a_path=args.model,
                       a_grid_search=args.grid_search
                       )
        LOGGER.debug("Analyzer trained.")
    else:
        analyzer = SentimentAnalyzer.load(args.model,
                                          args.w2v_path,
                                          on_demand=args.batch)
        if args.mode == DEBUG:
            for tweet_i in _read_data(args.files):
                tweet_i.label = analyzer.debug(tweet_i)
                print(str(tweet_i))
        # batch mode is only used in the testing mode
        elif args.batch:
            tweets = [tweet_i
                      for tweet_i in _read_data(args.files, False)]
            analyzer.batch_predict(tweets)
            for tweet_i in tweets:
                print(str(tweet_i))
        else:
            for tweet_i in _read_data(args.files):
                tweet_i.label = analyzer.predict(tweet_i)
                print(str(tweet_i))


##################################################################
# Main
if __name__ == "__main__":
    main(sys.argv[1:])
